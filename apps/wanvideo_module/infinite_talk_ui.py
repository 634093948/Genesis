#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Infinite Talk UI Integration
Infinite Talk UI é›†æˆ

Based on: Infinite Talk test(1).json workflow
åŠŸèƒ½: å›¾åƒ + éŸ³é¢‘ -> è¯´è¯è§†é¢‘

Author: eddy
Date: 2025-11-16
"""

import sys
from pathlib import Path
import gradio as gr

# Add project root
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Import model utilities
from .model_utils import get_wanvideo_models, format_model_choices, WANVIDEO_SCHEDULERS
from .optimization_settings import create_optimization_settings

# Import Infinite Talk pipeline
try:
    import importlib.util
    pipeline_file = project_root / "apps" / "wanvideo_module" / "infinite_talk_pipeline.py"
    spec = importlib.util.spec_from_file_location("infinite_talk_pipeline", pipeline_file)
    pipeline_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(pipeline_module)
    
    InfiniteTalkPipeline = pipeline_module.InfiniteTalkPipeline
    WANVIDEO_AVAILABLE = pipeline_module.WANVIDEO_AVAILABLE
except Exception as e:
    print(f"Failed to import Infinite Talk pipeline: {e}")
    WANVIDEO_AVAILABLE = False


def create_infinite_talk_tab():
    """Create Infinite Talk UI tab"""
    
    if not WANVIDEO_AVAILABLE:
        gr.Markdown("""
        ## âš ï¸ Infinite Talk æš‚æ—¶ä¸å¯ç”¨
        
        ### åŸå› 
        WanVideo èŠ‚ç‚¹ä¾èµ– Tritonï¼Œä½† Windows ä¸Š Triton ä¸å¯ç”¨ã€‚
        
        ### è§£å†³æ–¹æ¡ˆ
        
        **æ¨èï¼šä½¿ç”¨å®Œæ•´ ComfyUI ç¯å¢ƒ**
        
        Infinite Talk å·¥ä½œæµå·²åœ¨ä»¥ä¸‹ç¯å¢ƒä¸­å¯ç”¨ï¼š
        ```
        E:\\liliyuanshangmie\\Fuxkcomfy_lris_kernel_gen2-4_speed_safe\\FuxkComfy\\
        ```
        
        è¯¥ç¯å¢ƒå·²æ­£ç¡®é…ç½®æ‰€æœ‰ä¾èµ–ã€‚
        
        **å·¥ä½œæµæ–‡ä»¶:**
        ```
        E:\\liliyuanshangmie\\Fuxkcomfy_lris_kernel_gen2-4_speed_safe\\FuxkComfy\\user\\default\\workflows\\Infinite Talk test(1).json
        ```
        
        ### å·²å®Œæˆçš„å·¥ä½œ
        - âœ… 11 ä¸ª custom_nodes åŒ…å·²å¤åˆ¶
        - âœ… ç®¡é“ä»£ç å·²åˆ›å»º
        - âœ… UI ä»£ç å·²åˆ›å»º
        - âœ… å·¥ä½œæµå·²åˆ†æ
        
        ### æŠ€æœ¯ç»†èŠ‚
        æŸ¥çœ‹æ–‡æ¡£: `docs/INFINITE_TALK_STATUS.md`
        
        ---
        
        **æç¤º**: å…¶ä»–åŠŸèƒ½ï¼ˆFlux, Qwen Image, WanVideo Generationï¼‰ä»ç„¶å¯ç”¨ï¼
        """)
        return
    
    # Create pipeline
    pipeline = InfiniteTalkPipeline()
    
    # Get available models
    available_models = get_wanvideo_models()
    
    gr.Markdown("""
    # ğŸ¤ Infinite Talk - è¯´è¯è§†é¢‘ç”Ÿæˆ
    
    **ä»å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆè¯´è¯è§†é¢‘ï¼ˆMultiTalkï¼‰**
    
    åŸºäº ComfyUI Infinite Talk å·¥ä½œæµ
    """)
    
    with gr.Row():
        # Left column: Settings
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ”§ æ¨¡å‹è®¾ç½®")
            
            with gr.Accordion("åŠ è½½æ¨¡å‹", open=True):
                model_name = gr.Dropdown(
                    label="WanVideo æ¨¡å‹",
                    choices=format_model_choices(available_models['diffusion_models']),
                    value=available_models['diffusion_models'][0] if available_models['diffusion_models'] else "wan2_1_dit.safetensors",
                    allow_custom_value=True,
                    info="ä» models/diffusion_models æˆ– models/unet æ‰«æ"
                )
                
                vae_name = gr.Dropdown(
                    label="VAE æ¨¡å‹",
                    choices=format_model_choices(available_models['vae']),
                    value=available_models['vae'][0] if available_models['vae'] else "Wan2_1_VAE_bf16.safetensors",
                    allow_custom_value=True,
                    info="ä» models/vae æ‰«æ"
                )
                
                t5_model = gr.Dropdown(
                    label="T5 æ–‡æœ¬ç¼–ç å™¨",
                    choices=format_model_choices(available_models['text_encoders']) + ["google/umt5-xxl"],
                    value="google/umt5-xxl",
                    allow_custom_value=True,
                    info="ä» models/text_encoders æˆ– models/clip æ‰«æï¼Œæˆ–ä½¿ç”¨ HuggingFace æ¨¡å‹å"
                )
                
                clip_vision = gr.Dropdown(
                    label="CLIP Vision æ¨¡å‹",
                    choices=format_model_choices(available_models['clip_vision']),
                    value=available_models['clip_vision'][0] if available_models['clip_vision'] else "clip_vision_g.safetensors",
                    allow_custom_value=True,
                    info="ä» models/clip_vision æ‰«æ"
                )
                
                # Build Wav2Vec choices: local models + HuggingFace models
                local_audio_models = available_models.get('audio_encoders', [])
                huggingface_models = [
                    "facebook/wav2vec2-base-960h",
                    "facebook/wav2vec2-large-960h",
                    "facebook/wav2vec2-large-960h-lv60-self"
                ]
                wav2vec_choices = local_audio_models + huggingface_models
                
                # Set default value
                if local_audio_models:
                    wav2vec_default = local_audio_models[0]
                else:
                    wav2vec_default = "facebook/wav2vec2-base-960h"
                
                wav2vec_model = gr.Dropdown(
                    label="Wav2Vec æ¨¡å‹",
                    choices=wav2vec_choices if wav2vec_choices else ["facebook/wav2vec2-base-960h"],
                    value=wav2vec_default,
                    allow_custom_value=True,
                    info="éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹ï¼ˆæœ¬åœ°: models/audio_encodersï¼Œæˆ– HuggingFace æ¨¡å‹åï¼‰"
                )
                
                load_btn = gr.Button("ğŸ“¥ åŠ è½½æ¨¡å‹", variant="secondary")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€",
                    value="æœªåŠ è½½æ¨¡å‹",
                    interactive=False,
                    lines=3
                )
            
            gr.Markdown("### ğŸ”§ æ¨¡å‹åŠ è½½é«˜çº§å‚æ•°")
            
            with gr.Row():
                model_quantization = gr.Dropdown(
                    label="æ¨¡å‹é‡åŒ–",
                    choices=[
                        "disabled",
                        "fp8_e4m3fn",
                        "fp8_e4m3fn_fast",
                        "fp8_e4m3fn_scaled",
                        "fp8_e5m2",
                        "fp8_e5m2_fast",
                        "fp8_e5m2_scaled",
                        "fp4_experimental",
                        "fp4_scaled",
                        "fp4_scaled_fast"
                    ],
                    value="fp4_scaled",
                    info="æ¨¡å‹é‡åŒ–æ–¹å¼ï¼ˆä¸ComfyUIèŠ‚ç‚¹å®Œå…¨åŒ¹é…ï¼‰"
                )
                model_attention = gr.Dropdown(
                    label="æ³¨æ„åŠ›æ¨¡å¼",
                    choices=["default", "sageattn", "sageattn_3", "sageattn_3_fp4"],
                    value="sageattn_3_fp4",
                    info="æ³¨æ„åŠ›è®¡ç®—æ¨¡å¼ï¼ˆsageattn_3_fp4 é…åˆ fp4_scaledï¼‰"
                )
            
            with gr.Row():
                vae_precision = gr.Dropdown(
                    label="VAE ç²¾åº¦",
                    choices=["fp32", "fp16", "bf16"],
                    value="bf16",
                    info="VAE æ¨¡å‹ç²¾åº¦"
                )
                model_precision = gr.Dropdown(
                    label="æ¨¡å‹ç²¾åº¦",
                    choices=["fp32", "fp16", "bf16"],
                    value="bf16",
                    info="ä¸»æ¨¡å‹åŸºç¡€ç²¾åº¦"
                )
            
            gr.Markdown("### ğŸ“ è¾“å…¥æ–‡ä»¶")
            
            image_input = gr.Image(
                label="è¾“å…¥å›¾åƒ",
                type="filepath",
                sources=["upload"]
            )
            
            audio_input = gr.Audio(
                label="è¾“å…¥éŸ³é¢‘",
                type="filepath",
                sources=["upload"]
            )
            
            gr.Markdown("### ğŸ“ æç¤ºè¯")
            
            prompt = gr.Textbox(
                label="æ­£å‘æç¤ºè¯",
                placeholder="æè¿°è§†é¢‘å†…å®¹...",
                lines=3
            )
            
            negative_prompt = gr.Textbox(
                label="è´Ÿå‘æç¤ºè¯",
                placeholder="æè¿°ä¸æƒ³è¦çš„å†…å®¹...",
                lines=2,
                value="worst quality, low quality, blurry, distorted"
            )
            
            gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
            
            with gr.Row():
                width = gr.Slider(
                    label="å®½åº¦",
                    minimum=256,
                    maximum=1024,
                    value=768,
                    step=64
                )
                
                height = gr.Slider(
                    label="é«˜åº¦",
                    minimum=256,
                    maximum=1024,
                    value=768,
                    step=64
                )
            
            with gr.Row():
                video_length = gr.Slider(
                    label="è§†é¢‘é•¿åº¦ï¼ˆå¸§æ•°ï¼‰",
                    minimum=1,
                    maximum=200,
                    value=49,
                    step=1
                )
                
                fps = gr.Slider(
                    label="å¸§ç‡ (FPS)",
                    minimum=1,
                    maximum=60,
                    value=8,
                    step=1
                )
            
            with gr.Row():
                steps = gr.Slider(
                    label="é‡‡æ ·æ­¥æ•°",
                    minimum=1,
                    maximum=100,
                    value=30,
                    step=1,
                    info="æ³¨æ„ï¼šMultiTalk æ¨¡å¼å›ºå®šä½¿ç”¨ 4 æ­¥é‡‡æ · [1000, 750, 500, 250]ï¼Œæ­¤å‚æ•°ä»…ç”¨äºå…¶ä»–æ¨¡å¼"
                )
                
                cfg = gr.Slider(
                    label="CFG Scale",
                    minimum=0.0,
                    maximum=20.0,
                    value=7.0,
                    step=0.5
                )
            
            with gr.Row():
                sampler = gr.Dropdown(
                    label="é‡‡æ ·å™¨ (Sampler)",
                    choices=["euler", "euler_a", "heun", "dpm_2", "dpm_2_a", "lms", "dpmpp_2m", "dpmpp_sde"],
                    value="euler",
                    info="åŸºç¡€é‡‡æ ·å™¨ï¼ˆæŸäº›è°ƒåº¦å™¨ä¼šå¿½ç•¥æ­¤é¡¹ï¼‰"
                )
                
                scheduler = gr.Dropdown(
                    label="è°ƒåº¦å™¨ (Scheduler)",
                    choices=WANVIDEO_SCHEDULERS,
                    value="multitalk",
                    info="WanVideo ä¸“ç”¨è°ƒåº¦å™¨ï¼Œæ¨è multitalk"
                )
            
            with gr.Row():
                shift = gr.Slider(
                    label="Shift å‚æ•°",
                    info="æ—¶é—´æ­¥åç§»é‡ï¼Œå½±å“ç”Ÿæˆè´¨é‡",
                    minimum=0.0,
                    maximum=10.0,
                    value=1.0,
                    step=0.1
                )
            
            seed = gr.Number(
                label="ç§å­ (-1 ä¸ºéšæœº)",
                value=-1,
                precision=0
            )
            
            gr.Markdown("### ğŸµ éŸ³é¢‘å‚æ•°")
            
            with gr.Row():
                audio_num_frames = gr.Slider(
                    label="éŸ³é¢‘å¸§æ•° (num_frames)",
                    info="ç”¨äºè®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼Œå·¥ä½œæµé»˜è®¤33",
                    minimum=1,
                    maximum=200,
                    value=33,
                    step=1
                )
                
                normalize_loudness = gr.Checkbox(
                    label="å½’ä¸€åŒ–éŸ³é‡",
                    value=True
                )
            
            with gr.Row():
                audio_scale = gr.Slider(
                    label="éŸ³é¢‘å¼ºåº¦ (audio_scale)",
                    info="éŸ³é¢‘æ¡ä»¶å¼ºåº¦",
                    minimum=0.0,
                    maximum=10.0,
                    value=1.0,
                    step=0.1
                )
                
                audio_cfg_scale = gr.Slider(
                    label="éŸ³é¢‘CFG (audio_cfg_scale)",
                    info="éŸ³é¢‘CFGç¼©æ”¾",
                    minimum=0.0,
                    maximum=10.0,
                    value=1.0,
                    step=0.1
                )
            
            gr.Markdown("### ğŸ¬ è§†é¢‘ç”Ÿæˆå‚æ•°")
            
            with gr.Row():
                motion_frame = gr.Slider(
                    label="è¿åŠ¨å¸§ (motion_frame)",
                    info="é‡å å¸§é•¿åº¦",
                    minimum=1,
                    maximum=100,
                    value=25,
                    step=1
                )
                
                colormatch = gr.Dropdown(
                    label="é¢œè‰²åŒ¹é… (colormatch)",
                    choices=["disabled", "mkl", "hm", "reinhard", "mvgd", "hm-mvgd-hm", "hm-mkl-hm"],
                    value="mkl",
                    info="çª—å£é—´é¢œè‰²åŒ¹é…æ–¹æ³•"
                )
            
            gr.Markdown("### ğŸ–¼ï¸ å›¾åƒé¢„å¤„ç†")
            
            with gr.Row():
                use_image_resize = gr.Checkbox(
                    label="å¯ç”¨å›¾åƒç¼©æ”¾",
                    value=True,
                    info="ä½¿ç”¨ ImageResizeKJ è¿›è¡Œé«˜è´¨é‡ç¼©æ”¾"
                )
                resize_interpolation = gr.Dropdown(
                    label="æ’å€¼æ–¹æ³•",
                    choices=["lanczos", "bicubic", "bilinear", "nearest"],
                    value="lanczos",
                    info="ç¼©æ”¾æ’å€¼ç®—æ³•"
                )
            
            with gr.Row():
                resize_method = gr.Dropdown(
                    label="ç¼©æ”¾æ–¹æ³•",
                    choices=["stretch", "keep proportion", "fill / crop", "pad"],
                    value="stretch",
                    info="å¦‚ä½•å¤„ç†å®½é«˜æ¯”"
                )
                resize_condition = gr.Dropdown(
                    label="ç¼©æ”¾æ¡ä»¶",
                    choices=["always", "downscale if bigger", "upscale if smaller", "if bigger area", "if smaller area"],
                    value="always",
                    info="ä½•æ—¶æ‰§è¡Œç¼©æ”¾"
                )
            
            gr.Markdown("### ğŸµ éŸ³é¢‘é¢„å¤„ç†")
            
            with gr.Row():
                enable_audio_crop = gr.Checkbox(
                    label="å¯ç”¨éŸ³é¢‘è£å‰ª",
                    value=False,
                    info="è£å‰ªéŸ³é¢‘åˆ°æŒ‡å®šæ—¶é—´æ®µ"
                )
                audio_start_time = gr.Slider(
                    label="å¼€å§‹æ—¶é—´ (ç§’)",
                    minimum=0,
                    maximum=60,
                    value=0,
                    step=0.1
                )
            
            with gr.Row():
                audio_crop_duration = gr.Slider(
                    label="è£å‰ªæ—¶é•¿ (ç§’)",
                    minimum=0,
                    maximum=60,
                    value=0,
                    step=0.1,
                    info="0 è¡¨ç¤ºåˆ°éŸ³é¢‘ç»“å°¾"
                )
                enable_audio_separation = gr.Checkbox(
                    label="å¯ç”¨éŸ³é¢‘åˆ†ç¦»",
                    value=False,
                    info="åˆ†ç¦»äººå£°å’ŒèƒŒæ™¯éŸ³ï¼ˆä»…ä¿ç•™äººå£°ï¼‰"
                )
            
            with gr.Row():
                separation_model = gr.Dropdown(
                    label="åˆ†ç¦»æ¨¡å‹",
                    choices=["UVR-MDX-NET-Inst_HQ_3", "UVR_MDXNET_KARA_2", "Kim_Vocal_2"],
                    value="UVR-MDX-NET-Inst_HQ_3",
                    info="éŸ³é¢‘åˆ†ç¦»ä½¿ç”¨çš„æ¨¡å‹"
                )
            
            gr.Markdown("### ğŸ”¢ è‡ªåŠ¨å‚æ•°è®¡ç®—")
            
            with gr.Row():
                auto_calculate_frames = gr.Checkbox(
                    label="æ ¹æ®éŸ³é¢‘æ—¶é•¿è‡ªåŠ¨è®¡ç®—å¸§æ•°",
                    value=True,
                    info="ä½¿ç”¨éŸ³é¢‘æ—¶é•¿ Ã— FPS è‡ªåŠ¨è®¡ç®—è§†é¢‘å¸§æ•°"
                )
                max_frames = gr.Slider(
                    label="æœ€å¤§å¸§æ•°é™åˆ¶",
                    minimum=1,
                    maximum=500,
                    value=200,
                    step=1,
                    info="è‡ªåŠ¨è®¡ç®—æ—¶çš„ä¸Šé™"
                )
            
            # Use shared optimization settings
            opt_components = create_optimization_settings(default_blocks=20, show_vae_blocks=True)
            
            generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
        
        # Right column: Output
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ¬ ç”Ÿæˆç»“æœ")
            
            output_video = gr.Video(
                label="è¾“å‡ºè§†é¢‘"
            )
            
            output_info = gr.Markdown(
                value="åŠ è½½æ¨¡å‹å¹¶ä¸Šä¼ æ–‡ä»¶åç‚¹å‡»ç”Ÿæˆ..."
            )
    
    # Event handlers
    def load_models_wrapper(model_name, vae_name, t5_model, clip_vision, wav2vec_model,
                           model_quantization, model_attention, vae_precision, model_precision):
        """Load models wrapper"""
        try:
            if not WANVIDEO_AVAILABLE:
                return "âŒ WanVideo æ¨¡å—æœªåŠ è½½"
            
            success = pipeline.load_models(
                model_name=model_name,
                vae_name=vae_name,
                t5_model_name=t5_model,
                clip_vision_name=clip_vision,
                wav2vec_model_name=wav2vec_model,
                model_quantization=model_quantization,
                model_attention=model_attention,
                vae_precision=vae_precision,
                model_precision=model_precision
            )
            
            if success:
                return f"""âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!

**å·²åŠ è½½:**
- WanVideo: {model_name}
- VAE: {vae_name}
- T5: {t5_model}
- CLIP Vision: {clip_vision}
- Wav2Vec: {wav2vec_model}

ç°åœ¨å¯ä»¥ç”Ÿæˆè§†é¢‘äº†ï¼
"""
            else:
                return "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒæŸ¥çœ‹æ§åˆ¶å°äº†è§£è¯¦æƒ…"
                
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ é”™è¯¯: {e}"
    
    def generate_wrapper(
        image_path, audio_path, prompt, negative_prompt,
        width, height, video_length, steps, cfg,
        sampler, scheduler, shift, seed, fps,
        audio_num_frames, normalize_loudness, audio_scale, audio_cfg_scale,
        motion_frame, colormatch,
        use_image_resize, resize_interpolation, resize_method, resize_condition,
        enable_audio_crop, audio_start_time, audio_crop_duration,
        enable_audio_separation, separation_model,
        auto_calculate_frames, max_frames,
        blocks_to_swap, vae_blocks_to_swap,
        enable_cuda_optimization, enable_dram_optimization,
        auto_hardware_tuning, vram_threshold_percent,
        num_cuda_streams, bandwidth_target,
        offload_txt_emb, offload_img_emb, debug_mode,
        progress=gr.Progress()
    ):
        """Generate video wrapper"""
        try:
            if not WANVIDEO_AVAILABLE:
                return None, "âŒ WanVideo æ¨¡å—æœªåŠ è½½"
            
            if not image_path:
                return None, "âŒ è¯·ä¸Šä¼ å›¾åƒ"
            
            if not audio_path:
                return None, "âŒ è¯·ä¸Šä¼ éŸ³é¢‘"
            
            progress(0, desc="ç”Ÿæˆä¸­...")
            
            # Build optimization args
            optimization_args = {
                'blocks_to_swap': int(blocks_to_swap),
                'vae_blocks_to_swap': int(vae_blocks_to_swap),
                'enable_cuda_optimization': enable_cuda_optimization,
                'enable_dram_optimization': enable_dram_optimization,
                'auto_hardware_tuning': auto_hardware_tuning,
                'vram_threshold_percent': vram_threshold_percent,
                'num_cuda_streams': int(num_cuda_streams),
                'bandwidth_target': bandwidth_target,
                'offload_txt_emb': offload_txt_emb,
                'offload_img_emb': offload_img_emb,
                'debug_mode': debug_mode,
            }
            
            # Generate
            output_path = pipeline.generate(
                image_path=image_path,
                audio_path=audio_path,
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=int(width),
                height=int(height),
                video_length=int(video_length),
                steps=int(steps),
                cfg=cfg,
                sampler_name=sampler,
                scheduler=scheduler,
                shift=shift,
                seed=int(seed),
                fps=int(fps),
                audio_num_frames=int(audio_num_frames),
                audio_scale=audio_scale,
                audio_cfg_scale=audio_cfg_scale,
                normalize_loudness=normalize_loudness,
                motion_frame=int(motion_frame),
                colormatch=colormatch,
                use_image_resize=use_image_resize,
                resize_interpolation=resize_interpolation,
                resize_method=resize_method,
                resize_condition=resize_condition,
                enable_audio_crop=enable_audio_crop,
                audio_start_time=audio_start_time,
                audio_crop_duration=audio_crop_duration,
                enable_audio_separation=enable_audio_separation,
                separation_model=separation_model,
                auto_calculate_frames=auto_calculate_frames,
                max_frames=int(max_frames),
                optimization_args=optimization_args
            )
            
            if output_path:
                info = f"""
## âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ!

**å‚æ•°:**
- å°ºå¯¸: {width} x {height}
- å¸§æ•°: {video_length}
- FPS: {fps}
- æ­¥æ•°: {steps}
- CFG: {cfg}
- é‡‡æ ·å™¨: {sampler}
- è°ƒåº¦å™¨: {scheduler}
- Shift: {shift}
- ç§å­: {seed if seed >= 0 else 'éšæœº'}
- BlockSwap: {blocks_to_swap} å—

**è¾“å‡º:** {output_path}
"""
                return output_path, info
            else:
                return None, "âŒ ç”Ÿæˆå¤±è´¥ï¼ŒæŸ¥çœ‹æ§åˆ¶å°äº†è§£è¯¦æƒ…"
                
        except Exception as e:
            import traceback
            traceback.print_exc()
            return None, f"âŒ é”™è¯¯: {e}"
    
    # Connect events
    load_btn.click(
        fn=load_models_wrapper,
        inputs=[model_name, vae_name, t5_model, clip_vision, wav2vec_model,
                model_quantization, model_attention, vae_precision, model_precision],
        outputs=[model_status]
    )
    
    generate_btn.click(
        fn=generate_wrapper,
        inputs=[
            image_input, audio_input, prompt, negative_prompt,
            width, height, video_length, steps, cfg,
            sampler, scheduler, shift, seed, fps,
            audio_num_frames, normalize_loudness, audio_scale, audio_cfg_scale,
            motion_frame, colormatch,
            use_image_resize, resize_interpolation, resize_method, resize_condition,
            enable_audio_crop, audio_start_time, audio_crop_duration,
            enable_audio_separation, separation_model,
            auto_calculate_frames, max_frames,
            opt_components['blocks_to_swap'],
            opt_components.get('vae_blocks_to_swap', gr.Number(value=0)),
            opt_components['enable_cuda_optimization'],
            opt_components['enable_dram_optimization'],
            opt_components['auto_hardware_tuning'],
            opt_components['vram_threshold_percent'],
            opt_components['num_cuda_streams'],
            opt_components['bandwidth_target'],
            opt_components['offload_txt_emb'],
            opt_components['offload_img_emb'],
            opt_components['debug_mode']
        ],
        outputs=[output_video, output_info]
    )


if __name__ == "__main__":
    # Test UI
    with gr.Blocks() as demo:
        create_infinite_talk_tab()
    
    demo.launch()
