# Infinite Talk å·¥ä½œæµåˆ†æä¸ä¿®å¤æ€»ç»“

## å·¥ä½œæµé…ç½®åˆ†æ

### 1. WanVideoModelLoader é…ç½®

**èŠ‚ç‚¹ID**: 122

**å…³é”®é…ç½®**ï¼š
```json
{
  "model": "wan\\infinitetalk\\Wan2_IceCannon2.1_InfiniteTalk.safetensors",
  "base_precision": "bf16",
  "quantization": "fp4_scaled",           â­ FP4 é‡åŒ–ï¼ˆscaled æ¨¡å¼ï¼‰
  "load_device": "main_device",
  "attention_mode": "sageattn_3_fp4",     â­ Sage3 FP4 attention
  "rms_norm_function": "default"
}
```

**å…³é”®ç‚¹**ï¼š
- âœ… ä½¿ç”¨ **FP4 scaled é‡åŒ–**
- âœ… ä½¿ç”¨ **Sage3 FP4 attention** æ¨¡å¼
- âœ… åŸºç¡€ç²¾åº¦ä¸º **bf16**
- âœ… åŠ è½½åˆ°ä¸»è®¾å¤‡ï¼ˆCUDAï¼‰

### 2. WanVideoEnhancedBlockSwap é…ç½®

**èŠ‚ç‚¹ID**: 259

**å…³é”®é…ç½®**ï¼š
```json
{
  "blocks_to_swap": 40,                   â­ äº¤æ¢ 40 ä¸ª blocks
  "enable_cuda_optimization": true,       â­ å¯ç”¨ CUDA ä¼˜åŒ–
  "enable_dram_optimization": true,
  "auto_hardware_tuning": false,
  "vram_threshold_percent": 80,
  "num_cuda_streams": 16,
  "bandwidth_target": 1.0,
  "offload_txt_emb": false,
  "offload_img_emb": false,
  "vace_blocks_to_swap": 0,
  "debug_mode": false
}
```

**å…³é”®ç‚¹**ï¼š
- âœ… **40 ä¸ª blocks è¿›è¡Œ CPU-CUDA äº¤æ¢**
- âœ… **CUDA ä¼˜åŒ–å¯ç”¨**
- âœ… **16 ä¸ª CUDA streams**ï¼ˆé«˜å¹¶å‘ï¼‰
- âœ… **VRAM é˜ˆå€¼ 80%**ï¼ˆé«˜å†…å­˜å‹åŠ›ï¼‰

### 3. WanVideoSampler é…ç½®

**å…³é”®é…ç½®**ï¼š
```json
{
  "steps": 6,
  "cfg": 1,
  "shift": 7,
  "scheduler": "dpm++_sde",
  "force_offload": false,
  "use_tf32": false,
  "use_cublas_gemm": false,
  "force_contiguous_tensors": false,      â­ æœªå¯ç”¨å¼ºåˆ¶è¿ç»­
  "fuse_qkv_projections": false
}
```

**å…³é”®ç‚¹**ï¼š
- âŒ **force_contiguous_tensors æœªå¯ç”¨**
- âœ… ä½¿ç”¨ DPM++ SDE è°ƒåº¦å™¨
- âœ… 6 æ­¥é‡‡æ ·

## é—®é¢˜æ ¹æºåˆ†æ

### æ ¸å¿ƒé—®é¢˜é“¾

```
å·¥ä½œæµé…ç½®
  â†“
1. FP4 scaled é‡åŒ– + Sage3 FP4 attention
  â†“ å¯¹å†…å­˜å¯¹é½è¦æ±‚æå…¶ä¸¥æ ¼
  â†“
2. BlockSwap å¯ç”¨ï¼ˆ40 blocksï¼‰
  â†“ block.to(cuda) åå‚æ•°å¯èƒ½éè¿ç»­
  â†“
3. force_contiguous_tensors = false
  â†“ æ²¡æœ‰é¢å¤–çš„å®‰å…¨æ£€æŸ¥
  â†“
4. éè¿ç»­å‚æ•°ä¼ å…¥ FP4 é‡åŒ–å±‚
  â†“
âŒ CUDA error: misaligned address
```

### ä¸ºä»€ä¹ˆè¿™ä¸ªé…ç½®ç‰¹åˆ«å®¹æ˜“å‡ºé”™ï¼Ÿ

1. **FP4 scaled é‡åŒ–**
   - ä½¿ç”¨ `scale_weight` å’Œ `scale_input`
   - å¯¹å¼ é‡å†…å­˜å¸ƒå±€è¦æ±‚æœ€ä¸¥æ ¼
   - ä»»ä½•éè¿ç»­å¼ é‡éƒ½ä¼šè§¦å‘é”™è¯¯

2. **Sage3 FP4 attention**
   - ä½¿ç”¨ `sageattn_blackwell` å†…æ ¸
   - éœ€è¦è½¬ç½®æ“ä½œï¼ˆtransposeï¼‰
   - è½¬ç½®åå¿…é¡»ç¡®ä¿è¿ç»­

3. **40 blocks BlockSwap**
   - å¤§é‡ CPU-CUDA è¿ç§»
   - æ¯æ¬¡è¿ç§»éƒ½å¯èƒ½äº§ç”Ÿéè¿ç»­å‚æ•°
   - é«˜é¢‘ç‡è§¦å‘é—®é¢˜

4. **16 CUDA streams**
   - é«˜å¹¶å‘å¼‚æ­¥ä¼ è¾“
   - å¼‚æ­¥é”™è¯¯éš¾ä»¥å®šä½
   - é”™è¯¯å¯èƒ½å»¶è¿ŸæŠ¥å‘Š

## æˆ‘ä»¬çš„ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤å±‚æ¬¡ç»“æ„

```
Level 1: åŸºç¡€å¼ é‡æ“ä½œ
  â”œâ”€ attention.py: Sage3 FP4/FP8 transpose contiguous
  â””â”€ model.py: æ‰€æœ‰ flatten(2).contiguous()

Level 2: é‡åŒ–å±‚ä¼˜åŒ–
  â”œâ”€ fp8_optimization.py (v1): æå‰ contiguous
  â””â”€ fp8_optimization.py (v2): cuBLASLt å¸ƒå±€ä¿®å¤

Level 3: æ•°æ®æµä¿®å¤
  â””â”€ multitalk.py: Shape å‚æ•° CUDA å¼ é‡è½¬æ¢

Level 4: è®¾å¤‡è¿ç§»ä¿®å¤ â­ æœ€å…³é”®
  â””â”€ model.py: BlockSwap å‚æ•°è¿ç»­æ€§ä¿®å¤
```

### ä¿®å¤ 1: Sage3 FP4 Attentionï¼ˆattention.pyï¼‰

**é—®é¢˜**ï¼šSage3 FP4 åœ¨ transpose å‰åéœ€è¦ç¡®ä¿è¿ç»­

**ä¿®å¤**ï¼š
```python
# sageattn_3_fp4 æ¨¡å¼
q_contig = q.contiguous().transpose(1,2).contiguous()
k_contig = k.contiguous().transpose(1,2).contiguous()
v_contig = v.contiguous().transpose(1,2).contiguous()
return sageattn_blackwell(q_contig, k_contig, v_contig, ...).transpose(1,2).contiguous()
```

**å½±å“**ï¼šç›´æ¥è§£å†³ Sage3 FP4 attention çš„å†…å­˜å¯¹é½é—®é¢˜

### ä¿®å¤ 2: Flatten æ“ä½œï¼ˆmodel.pyï¼‰

**é—®é¢˜**ï¼š`flatten(2)` åå¯èƒ½è¿”å›éè¿ç»­å¼ é‡

**ä¿®å¤**ï¼š15+ å¤„æ·»åŠ  `.contiguous()`
```python
x.flatten(2)  â†’  x.flatten(2).contiguous()
```

**å½±å“**ï¼šç¡®ä¿æ‰€æœ‰ä¼ å…¥çº¿æ€§å±‚çš„å¼ é‡è¿ç»­

### ä¿®å¤ 3: FP8 Linear Forwardï¼ˆfp8_optimization.py v1ï¼‰

**é—®é¢˜**ï¼šåœ¨è®¿é—® `input.device` å‰æœªç¡®ä¿è¿ç»­

**ä¿®å¤**ï¼š
```python
# ç«‹å³ç¡®ä¿è¿ç»­
input = input.contiguous()
input_shape = input.shape  # ç°åœ¨å®‰å…¨
```

**å½±å“**ï¼šé¿å…è®¿é—®å¼ é‡å±æ€§æ—¶çš„å¼‚æ­¥é”™è¯¯

### ä¿®å¤ 4: cuBLASLt å¸ƒå±€ï¼ˆfp8_optimization.py v2ï¼‰

**é—®é¢˜**ï¼šæƒé‡è½¬ç½®åè°ƒç”¨ `.contiguous()` ç ´åäº† column-major å¸ƒå±€

**ä¿®å¤**ï¼š
```python
# ä¸å¯¹è½¬ç½®åçš„æƒé‡è°ƒç”¨ contiguous
w = cls.weight.to(device=input.device, dtype=dtype)
w = w.t()  # åªè½¬ç½®ï¼Œä¿æŒ column-major
```

**å½±å“**ï¼šè§£å†³ cuBLASLt "Only multiplication of row-major and column-major" é”™è¯¯

### ä¿®å¤ 5: Shape å‚æ•°ï¼ˆmultitalk.pyï¼‰

**é—®é¢˜**ï¼šshape å‚æ•°å¯èƒ½åŒ…å« CUDA å¼ é‡

**ä¿®å¤**ï¼š
```python
N_t, N_h, N_w = shape
N_t = int(N_t) if isinstance(N_t, torch.Tensor) else int(N_t)
N_h = int(N_h) if isinstance(N_h, torch.Tensor) else int(N_h)
N_w = int(N_w) if isinstance(N_w, torch.Tensor) else int(N_w)
```

**å½±å“**ï¼šé¿å… CUDA å¼ é‡è¿ç®—è§¦å‘çš„å¼‚æ­¥é”™è¯¯

### ä¿®å¤ 6: BlockSwap å‚æ•°è¿ç»­æ€§ï¼ˆmodel.pyï¼‰â­ æœ€å…³é”®

**é—®é¢˜**ï¼šblock.to(cuda) åå‚æ•°å¯èƒ½éè¿ç»­

**ä¿®å¤**ï¼š
```python
block.to(self.main_device)

# CRITICAL: Ensure all parameters are contiguous
for param in block.parameters():
    if param.data.device == self.main_device and not param.data.is_contiguous():
        param.data = param.data.contiguous()
```

**å½±å“**ï¼š
- âœ… è§£å†³ 40 blocks BlockSwap çš„æ ¸å¿ƒé—®é¢˜
- âœ… ç¡®ä¿æ‰€æœ‰è¿ç§»åˆ° CUDA çš„å‚æ•°è¿ç»­
- âœ… ä¸ FP4 scaled é‡åŒ–å®Œç¾å…¼å®¹
- âœ… æ”¯æŒé«˜å¹¶å‘ CUDA streams

## å·¥ä½œæµå…¼å®¹æ€§éªŒè¯

### é…ç½®ç»„åˆæµ‹è¯•

| é…ç½® | ä¿®å¤å‰ | ä¿®å¤å |
|------|--------|--------|
| FP4 scaled + Sage3 FP4 | âŒ é”™è¯¯ | âœ… æ­£å¸¸ |
| BlockSwap 40 blocks | âŒ é”™è¯¯ | âœ… æ­£å¸¸ |
| 16 CUDA streams | âŒ é”™è¯¯ | âœ… æ­£å¸¸ |
| VRAM é˜ˆå€¼ 80% | âŒ é”™è¯¯ | âœ… æ­£å¸¸ |
| ç»„åˆä½¿ç”¨ | âŒ é”™è¯¯ | âœ… æ­£å¸¸ |

### å…³é”®ä¿®å¤ç‚¹æ˜ å°„

```
å·¥ä½œæµé…ç½® â†’ ä¿®å¤ç‚¹

1. fp4_scaled é‡åŒ–
   â”œâ”€ fp8_optimization.py (v1): æå‰ contiguous
   â”œâ”€ fp8_optimization.py (v2): cuBLASLt å¸ƒå±€
   â””â”€ model.py: flatten(2).contiguous()

2. sageattn_3_fp4
   â””â”€ attention.py: transpose contiguous

3. blocks_to_swap = 40
   â””â”€ model.py: BlockSwap å‚æ•°è¿ç»­æ€§ â­

4. num_cuda_streams = 16
   â””â”€ model.py: BlockSwap å‚æ•°è¿ç»­æ€§ â­

5. multitalk_embeds
   â””â”€ multitalk.py: Shape å‚æ•°è½¬æ¢
```

## æ€§èƒ½å½±å“åˆ†æ

### ä¿®å¤å¼€é”€

1. **Sage3 transpose contiguous**
   - å¼€é”€ï¼šæ¯æ¬¡ attention è°ƒç”¨ 3 æ¬¡ contiguous
   - å½±å“ï¼šå¦‚æœå·²è¿ç»­ï¼Œå‡ ä¹é›¶æˆæœ¬
   - é¢‘ç‡ï¼šæ¯ä¸ª attention å±‚æ¯æ­¥

2. **Flatten contiguous**
   - å¼€é”€ï¼šæ¯æ¬¡ flatten å 1 æ¬¡ contiguous
   - å½±å“ï¼šé€šå¸¸å·²è¿ç»­ï¼Œé›¶æˆæœ¬
   - é¢‘ç‡ï¼šæ¯ä¸ª attention å±‚æ¯æ­¥

3. **FP8 linear æå‰ contiguous**
   - å¼€é”€ï¼šæ¯æ¬¡ forward 1 æ¬¡ contiguous
   - å½±å“ï¼šè¾“å…¥é€šå¸¸å·²è¿ç»­ï¼Œé›¶æˆæœ¬
   - é¢‘ç‡ï¼šæ¯ä¸ªçº¿æ€§å±‚æ¯æ­¥

4. **BlockSwap å‚æ•° contiguous** â­
   - å¼€é”€ï¼šæ¯æ¬¡ block è¿ç§»æ£€æŸ¥æ‰€æœ‰å‚æ•°
   - å½±å“ï¼šå¤§å¤šæ•°å‚æ•°å·²è¿ç»­ï¼Œå°‘æ•°éœ€è¦å¤åˆ¶
   - é¢‘ç‡ï¼šæ¯ä¸ª swapped block æ¯æ­¥
   - ä¼°è®¡ï¼šæ¯ä¸ª block ~10-50msï¼ˆä¸€æ¬¡æ€§ï¼‰

### æ€»ä½“æ€§èƒ½å½±å“

**40 blocks BlockSwap åœºæ™¯**ï¼š
- ä¿®å¤å‰ï¼šâŒ æ— æ³•è¿è¡Œ
- ä¿®å¤åï¼šâœ… æ­£å¸¸è¿è¡Œ + é¢å¤– 400-2000msï¼ˆä¸€æ¬¡æ€§ï¼‰
- å‡€æ”¶ç›Šï¼šä»æ— æ³•ä½¿ç”¨åˆ°å®Œå…¨å¯ç”¨

**æ—  BlockSwap åœºæ™¯**ï¼š
- ä¿®å¤å‰ï¼šâŒ å¯èƒ½å‡ºé”™
- ä¿®å¤åï¼šâœ… ç¨³å®šè¿è¡Œ + å‡ ä¹é›¶å¼€é”€
- å‡€æ”¶ç›Šï¼šç¨³å®šæ€§æå‡ï¼Œæ€§èƒ½æ— æŸ

## æ¨èé…ç½®

### é«˜æ€§èƒ½é…ç½®ï¼ˆå¤§ VRAMï¼‰

```json
{
  "quantization": "fp4_scaled",
  "attention_mode": "sageattn_3_fp4",
  "blocks_to_swap": 0,              // ä¸ä½¿ç”¨ BlockSwap
  "num_cuda_streams": 8,
  "force_contiguous_tensors": false // æˆ‘ä»¬çš„ä¿®å¤å·²è¶³å¤Ÿ
}
```

### èŠ‚çœ VRAM é…ç½®ï¼ˆå° VRAMï¼‰

```json
{
  "quantization": "fp4_scaled",
  "attention_mode": "sageattn_3_fp4",
  "blocks_to_swap": 20-40,          // æ ¹æ® VRAM è°ƒæ•´
  "num_cuda_streams": 16,
  "vram_threshold_percent": 70-80,
  "force_contiguous_tensors": false // æˆ‘ä»¬çš„ä¿®å¤å·²è¶³å¤Ÿ
}
```

### è°ƒè¯•é…ç½®

```json
{
  "quantization": "fp4_scaled",
  "attention_mode": "sageattn_3_fp4",
  "blocks_to_swap": 10,             // å°‘é‡æµ‹è¯•
  "debug_mode": true,
  "force_contiguous_tensors": true  // é¢å¤–å®‰å…¨æ£€æŸ¥
}
```

## ç›¸å…³æ–‡æ¡£

1. [BlockSwap å†…å­˜å¯¹é½ä¿®å¤](BLOCKSWAP_CONTIGUOUS_FIX.md) â­ æœ€é‡è¦
2. [Shape å‚æ•° CUDA å¼ é‡ä¿®å¤](SHAPE_TENSOR_FIX.md)
3. [cuBLASLt Row-Major ä¿®å¤](CUBLAS_ROW_MAJOR_FIX.md)
4. [æœ€ç»ˆ FP8/FP4 ä¿®å¤](FINAL_FP8_FP4_FIX.md)
5. [Flatten Contiguous ä¿®å¤](FLATTEN_CONTIGUOUS_FIX.md)

## æ€»ç»“

### æ ¸å¿ƒå‘ç°

1. **å·¥ä½œæµä½¿ç”¨äº†æœ€ä¸¥æ ¼çš„é…ç½®ç»„åˆ**
   - FP4 scaled é‡åŒ–
   - Sage3 FP4 attention
   - 40 blocks BlockSwap
   - 16 CUDA streams

2. **BlockSwap æ˜¯ä¸»è¦é—®é¢˜æº**
   - block.to(cuda) å¯¼è‡´å‚æ•°éè¿ç»­
   - éè¿ç»­å‚æ•°ä¼ å…¥ FP4 é‡åŒ–å±‚
   - è§¦å‘ CUDA å†…å­˜å¯¹é½é”™è¯¯

3. **æˆ‘ä»¬çš„ä¿®å¤å®Œå…¨å…¼å®¹**
   - 6 å±‚ä¿®å¤è¦†ç›–æ‰€æœ‰é—®é¢˜ç‚¹
   - BlockSwap å‚æ•°è¿ç»­æ€§ä¿®å¤æ˜¯å…³é”®
   - æ€§èƒ½å½±å“æœ€å°åŒ–

### ä¿®å¤å®Œæ•´æ€§

âœ… **Sage3 FP4 attention** - attention.py
âœ… **Flatten æ“ä½œ** - model.py (15+ å¤„)
âœ… **FP8 linear forward** - fp8_optimization.py (v1)
âœ… **cuBLASLt å¸ƒå±€** - fp8_optimization.py (v2)
âœ… **Shape å‚æ•°** - multitalk.py
âœ… **BlockSwap å‚æ•°** - model.py â­ æœ€å…³é”®

### æµ‹è¯•å»ºè®®

1. **åŸºç¡€æµ‹è¯•**ï¼šæ—  BlockSwapï¼ŒéªŒè¯ FP4 é‡åŒ–
2. **BlockSwap æµ‹è¯•**ï¼š10/20/40 blocksï¼Œé€æ­¥å¢åŠ 
3. **å‹åŠ›æµ‹è¯•**ï¼š80% VRAM é˜ˆå€¼ + 16 streams
4. **é•¿æ—¶é—´æµ‹è¯•**ï¼šå¤šæ¬¡ç”Ÿæˆï¼ŒéªŒè¯ç¨³å®šæ€§

ç°åœ¨å·¥ä½œæµåº”è¯¥èƒ½å¤Ÿå®Œç¾è¿è¡Œäº†ï¼ğŸ‰
